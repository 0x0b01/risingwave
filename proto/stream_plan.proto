syntax = "proto3";

package stream_plan;

option java_multiple_files = true;
option java_package = "com.risingwave.proto.streaming.plan";
option optimize_for = SPEED;

import "expr.proto";
import "plan.proto";

message TableSourceNode {
  enum SourceType {
    TABLE = 0;
    STREAM = 1;
  }
  plan.TableRefId table_ref_id = 1;
  repeated int32 column_ids = 2;
  SourceType source_type = 3;
}

message ProjectNode {
  repeated expr.ExprNode select_list = 1;
}

message FilterNode {
  expr.ExprNode search_condition = 1;
}

// A materialized view is regarded as a table,
// hence we copy the CreateTableNode definition in OLAP PlanNode.
// In addition, we also specify primary key to MV for efficient point lookup during update and deletion.
message MViewNode {
  plan.TableRefId table_ref_id = 1;
  repeated plan.ColumnDesc column_descs = 2;
  repeated int32 pk_indices = 3;
  repeated plan.ColumnOrder column_orders = 4;
}

// Remark by Yanghao: for both local and global we use the same node in the protobuf.
// Local and global aggregator distinguish with each other in their aggregation definition.
message SimpleAggNode {
  repeated expr.AggCall agg_calls = 1;
}

message HashAggNode {
  repeated expr.InputRefExpr group_keys = 1;
  repeated expr.AggCall agg_calls = 2;
}

message TopNNode {
  repeated plan.ColumnOrder column_orders = 1;
  // 0 means no limit as limit of 0 means this node should be optimized away
  uint64 limit = 2;
  uint64 offset = 3;
}

message HashJoinNode {
  plan.JoinType join_type = 1;
  repeated int32 left_key = 2;
  repeated int32 right_key = 3;
}

message StreamNode {
  // Some nodes we can reuse plan nodes from plan.proto.
  // TODO: should we enforce strong typed body on nodes?
  oneof node {
    TableSourceNode table_source_node = 4;
    ProjectNode project_node = 5;
    FilterNode filter_node = 6;
    MViewNode mview_node = 7;
    SimpleAggNode simple_agg_node = 8;
    HashAggNode hash_agg_node = 9;
    TopNNode append_only_top_n_node = 10;
    HashJoinNode hash_join_node = 11;
    TopNNode top_n_node = 12;
  }
  // Child node in plan aka. upstream nodes in the streaming DAG
  repeated StreamNode input = 3;
  repeated uint32 pk_indices = 2;
}

// A dispatcher redistribute messages.
// We encode both the type and other usage information in the proto.
message Dispatcher {
  enum DispatcherType {
    SIMPLE = 0;
    ROUND_ROBIN = 1;
    HASH = 2;
    BROADCAST = 3;
    BLACKHOLE = 4;
  }
  DispatcherType type = 1;
  int32 column_idx = 2;
}

// A StreamFragment is a subgraph of the overall stream graph,
// we assume a fragment is always a chain for simplicity.
// That is, a fragment has exactly one head node and one sink node.
// We define the head node in the proto, and the upstream fragments in id
// as multiple fragments may share the same upstream fragment.
message StreamFragment {
  uint32 fragment_id = 1;
  StreamNode nodes = 2;
  // We assume the given upstream fragment are always previously defined,
  // i.e, the create stream requests must arrive in order.
  repeated Merger mergers = 3;
  Dispatcher dispatcher = 4;
  // Number of downstreams decides how many endpoints a dispatcher should dispatch.
  repeated uint32 downstream_fragment_id = 5;
  // The schema of input columns.
  repeated plan.ColumnDesc input_column_descs = 6;
}

// Remark: we DO need to explicitly encode mergers in the proto as it is necessary.
// A fragment can have multiple logical upstream and each logical upstream can have multiple physical fragments.
// For example, join.
message Merger {
  repeated uint32 upstream_fragment_id = 1;
}
