syntax = "proto3";

import "google/protobuf/any.proto";
import "expr.proto";
import "data.proto";
import "plan.proto";

package risingwave.streaming.proto;
option java_package = "com.risingwave.proto.streaming.plan";
option java_multiple_files = true;
option optimize_for = SPEED;

message TableSourceNode {
  risingwave.executor.proto.TableRefId table_ref_id = 1;
  repeated int32 column_ids = 2;
}

message ProjectNode {
  repeated risingwave.expr.proto.ExprNode select_list = 1;
}

message FilterNode {
  risingwave.expr.proto.ExprNode search_condition = 1;
}

message MemTableMaterializedViewNode {
  int32 memtable_id = 1;
  repeated int32 pk_idx = 2;
}

message LocalSimpleAggNode {
  risingwave.expr.proto.AggCall.Type aggregation_type = 1;
  risingwave.executor.proto.DataType input_type = 2;
  risingwave.executor.proto.DataType return_type = 3;
  int32 column_idx = 4;
}

message GlobalSimpleAggNode {
  risingwave.expr.proto.AggCall.Type aggregation_type = 1;
  risingwave.executor.proto.DataType input_type = 2;
  risingwave.executor.proto.DataType return_type = 3;
}

// Version 2 streaming nodes: MaterializedViewNode, SimpleAggNode, HashAggNode.

// A materialized view is regarded as a table,
// hence we copy the CreateTableNode definition in OLAP PlanNode.
message MaterializedViewNode{
  risingwave.executor.proto.TableRefId table_ref_id = 1;
  repeated risingwave.executor.proto.ColumnDesc column_descs = 2;
}

// Remark by Yanghao: for both local and global we use the same node in the protobuf.
// Local and global aggregator distinguish with each other in their aggregation definition.
message SimpleAggNode{
  repeated risingwave.expr.proto.AggCall agg_calls = 1;
}

message HashAggNode{
  repeated risingwave.expr.proto.InputRefExpr group_keys = 1;
  repeated risingwave.expr.proto.AggCall agg_calls = 2;
}

message StreamNode {
  enum StreamNodeType {
    TABLE_INGRESS = 0;
    KAFKA_INGRESS = 1;
    PROJECTION = 2;
    FILTER = 3;
    LOCAL_SIMPLE_AGG = 4;
    GLOBAL_SIMPLE_AGG = 5;
    LOCAL_HASH_AGG = 6;
    GLOBAL_HASH_AGG = 7;
    HASH_JOIN = 8;
    MEMTABLE_MATERIALIZED_VIEW = 9;
  }
  StreamNodeType node_type = 1;
  // Some nodes we can reuse plan nodes from plan.proto.
  // TODO: should we enforce strong typed body on nodes?
  google.protobuf.Any body = 2;
  // We assume that StreamNodes always appear in fragments,
  // and each fragment encode a chain of nodes.
  // Hence each StreamNode has only one downstream.
  StreamNode downstream_node = 3;
}

// A dispatcher redistribute messages.
// We encode both the type and other usage information in the proto.
message Dispatcher {
  enum DispatcherType {
    SIMPLE = 0;
    ROUND_ROBIN = 1;
    HASH = 2;
    BROADCAST = 3;
    BLACKHOLE = 4;
  }
  DispatcherType type = 1;
  int32 column_idx = 2;
}

// A StreamFragment is a subgraph of the overall stream graph,
// we assume a fragment is always a chain for simplicity.
// That is, a fragment has exactly one head node and one sink node.
// We define the head node in the proto, and the upstream fragments in id
// as multiple fragments may share the same upstream fragment.
message StreamFragment {
  uint32 fragment_id = 1;
  StreamNode nodes = 2;
  // We assume the given upstream fragment are always previously defined,
  // i.e, the create stream requests must arrive in order.
  repeated uint32 upstream_fragment_id = 3;
  Dispatcher dispatcher = 4;
  // Number of downstreams decides how many endpoints a dispatcher should dispatch.
  repeated uint32 downstream_fragment_id = 5;
}

// Remark: we do not explicitly encode mergers in the proto as it is not necessary.
// A fragment with multiple upstreams will automatically attach a merger in the front.
