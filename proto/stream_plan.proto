syntax = "proto3";

import "google/protobuf/any.proto";
import "expr.proto";
import "data.proto";
import "plan.proto";

package stream_plan;
option java_package = "com.risingwave.proto.streaming.plan";
option java_multiple_files = true;
option optimize_for = SPEED;

message TableSourceNode {
  plan.TableRefId table_ref_id = 1;
  repeated int32 column_ids = 2;
}

message ProjectNode {
  repeated expr.ExprNode select_list = 1;
}

message FilterNode {
  expr.ExprNode search_condition = 1;
}

// A materialized view is regarded as a table,
// hence we copy the CreateTableNode definition in OLAP PlanNode.
// In addition, we also specify primary key to MV for efficient point lookup during update and deletion.
message MViewNode {
  plan.TableRefId table_ref_id = 1;
  repeated plan.ColumnDesc column_descs = 2;
  repeated int32 pk_indices = 3;
}

// Remark by Yanghao: for both local and global we use the same node in the protobuf.
// Local and global aggregator distinguish with each other in their aggregation definition.
message SimpleAggNode {
  repeated expr.AggCall agg_calls = 1;
}

message HashAggNode {
  repeated expr.InputRefExpr group_keys = 1;
  repeated expr.AggCall agg_calls = 2;
}

message StreamNode {
  enum StreamNodeType {
    TABLE_INGRESS = 0;
    KAFKA_INGRESS = 1;
    PROJECTION = 2;
    FILTER = 3;
    LOCAL_SIMPLE_AGG = 4;
    GLOBAL_SIMPLE_AGG = 5;
    LOCAL_HASH_AGG = 6;
    GLOBAL_HASH_AGG = 7;
    HASH_JOIN = 8;
    MEMTABLE_MATERIALIZED_VIEW = 9;
  }
  StreamNodeType node_type = 1;
  // Some nodes we can reuse plan nodes from plan.proto.
  // TODO: should we enforce strong typed body on nodes?
  google.protobuf.Any body = 2;
  // Child node in plan aka. upstream nodes in the streaming DAG
  // Currently we assume the executors in one fragment are always chained list rather than tree
  StreamNode input = 3;
}

// A dispatcher redistribute messages.
// We encode both the type and other usage information in the proto.
message Dispatcher {
  enum DispatcherType {
    SIMPLE = 0;
    ROUND_ROBIN = 1;
    HASH = 2;
    BROADCAST = 3;
    BLACKHOLE = 4;
  }
  DispatcherType type = 1;
  int32 column_idx = 2;
}

// A StreamFragment is a subgraph of the overall stream graph,
// we assume a fragment is always a chain for simplicity.
// That is, a fragment has exactly one head node and one sink node.
// We define the head node in the proto, and the upstream fragments in id
// as multiple fragments may share the same upstream fragment.
message StreamFragment {
  uint32 fragment_id = 1;
  StreamNode nodes = 2;
  // We assume the given upstream fragment are always previously defined,
  // i.e, the create stream requests must arrive in order.
  repeated uint32 upstream_fragment_id = 3;
  Dispatcher dispatcher = 4;
  // Number of downstreams decides how many endpoints a dispatcher should dispatch.
  repeated uint32 downstream_fragment_id = 5;
}

// Remark: we do not explicitly encode mergers in the proto as it is not necessary.
// A fragment with multiple upstreams will automatically attach a merger in the front.
